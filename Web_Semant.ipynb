{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d214c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "833457a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jattioui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jattioui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlp_utils import *\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cc2e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_stopwords=['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren',\n",
    "               \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by',\n",
    "               'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don',\n",
    "               \"don't\", 'down', 'dtype', 'during', 'each', 'few', 'for', 'from', 'further', 'ha', 'had', 'hadn', \"hadn't\", \n",
    "               'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', \n",
    "               'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', \n",
    "               'length', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', \n",
    "               'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'object', 'of', 'off', 'on', 'once', 'only', 'or', 'other', \n",
    "               'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'say', 'shan', \"shan't\", 'she', \"she's\",\n",
    "               'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the',\n",
    "               'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to',\n",
    "               'too', 'u', 'under', 'until', 'up', 've', 'very', 'wa', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren',\n",
    "               \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\",\n",
    "               'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself',\n",
    "               'yourselves']\n",
    "\n",
    "import re\n",
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()\n",
    "\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "\n",
    "    ## Remove digits\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    ## remove mutliple space\n",
    "    text = re.sub(' +', ' ', text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15599822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52fa3507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(f1,f2):\n",
    "    return cosine(\n",
    "        tfidf_vectorizer.transform([f1]).toarray(),\n",
    "        tfidf_vectorizer.transform([f2]).toarray(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0119821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Entites(requet):\n",
    "    doc = nlp(requet)\n",
    "    sentences = [x for x in doc.sents]\n",
    "    print(sentences)\n",
    "    a=[(X.text.replace(' ','_')) for X in doc.ents]\n",
    "    entitie=[(x.lemma_[0].upper()+x.lemma_[1:]) for x in [y \n",
    "                                      for y\n",
    "                                      in nlp(str(sentences)) \n",
    "                                      if not y.is_stop and (y.pos_ == 'NOUN' or y.pos_ == 'PROPN' )]#or y.pos_ == 'PROPN'\n",
    "                                      ]\n",
    "    entities=a\n",
    "    for element in entitie:\n",
    "        if element not in entities:\n",
    "            entities.append(element)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6be09e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from rdflib import Graph\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON, N3\n",
    "lien1=[]\n",
    "entities2=[]\n",
    "sparql = SPARQLWrapper('https://dbpedia.org/sparql')\n",
    "def Requete(requetS):\n",
    "    global lien1\n",
    "    lien1=[]\n",
    "    data=[utils_preprocess_text(requetS, flg_stemm=False, flg_lemm=True)]\n",
    "    global resultat\n",
    "    global entities2\n",
    "    entities2=Entites(requetS)\n",
    "    #print(entities2)\n",
    "    seuilf=0.0\n",
    "    for entite in entities2:\n",
    "        \n",
    "        seuil=0.0\n",
    "        sparql.setQuery('''\n",
    "             PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "             PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "             PREFIX rd: <http://dbpedia.org/resource/>\n",
    "             PREFIX dbo:<http://dbpedia.org/ontology/>\n",
    "             SELECT DISTINCT ?object $abs ?comment \n",
    "             WHERE { rd:'''+ entite +'''  dbo:wikiPageWikiLink $object .\n",
    "             $object rdfs:comment $comment.\n",
    "             $object dbo:abstract $abs.\n",
    "             FILTER(lang(?abs)='en')\n",
    "             FILTER(lang(?comment)='en')\n",
    "            }\n",
    "        ''')\n",
    "        #print(entite)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        qres = sparql.query().convert()\n",
    "       \n",
    "        #print(qres)\n",
    "        for result in qres['results']['bindings']:\n",
    "            data.append(utils_preprocess_text(result['comment']['value'], flg_stemm=False, flg_lemm=True))\n",
    "            \n",
    "             #print(result['object']['value'],\" \\nAbstract is :\",result['comment']['value'])\n",
    "        tfidf_vector=tfidf_vectorizer.fit_transform(data)\n",
    "        \n",
    "        for result in qres['results']['bindings']:\n",
    "            dist=1-distance(utils_preprocess_text(requetS, flg_stemm=False, flg_lemm=True) ,utils_preprocess_text(result['comment']['value'], flg_stemm=False, flg_lemm=True))\n",
    "            # print(wv.wmdistance(requet ,result['comment']['value']))\n",
    "            if dist >seuil :\n",
    "                seuil=dist\n",
    "                #print(seuil)\n",
    "                resultat=result\n",
    "        #print(entite,resultat['object']['value'])\n",
    "        if not qres['results']['bindings'] :\n",
    "            lien1.append(\"this entity does not exist in dbpedia\")\n",
    "        else:\n",
    "            lien1.append(resultat['object']['value'])\n",
    "        #lang, value = result['object']['xml:lang'], result['object']['value']\n",
    "    #print(f'Lang: {vars}\\tValue: {subject}')\n",
    "    # if lang == 'en':\n",
    "        # print(value)\n",
    "#print(requet,resultatf['object']['value'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f705e8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sebastian Thrun works at Google]\n"
     ]
    }
   ],
   "source": [
    "import webbrowser\n",
    "def recherche():\n",
    "    req=entree.get()\n",
    "    doc = nlp(req[0].upper()+req[1:])\n",
    "    sentences = [x for x in doc.sents]\n",
    "    entity=[]\n",
    "    liens=[]\n",
    "    for sent in sentences:\n",
    "        Requete(str(sent))\n",
    "        entity.extend(entities2)\n",
    "        #print(entity)\n",
    "        liens.extend(lien1)\n",
    "    global canvas\n",
    "    canvas.destroy()\n",
    "    canvas = Canvas(fenetre, width=550, height=320)\n",
    "    canvas.pack() \n",
    "    link={}\n",
    "    for i in range(len(entity)):\n",
    "        b=Label(canvas , text=entity[i])\n",
    "        b.grid(row =i, column =0,padx=5, pady=5)\n",
    "        if i==0:\n",
    "            link = Label(canvas, text=liens[i], fg=\"#116562\")#cursor=\"hand2\"\n",
    "            link.grid(row =i, column =1,padx=5, pady=5)\n",
    "            link.bind(\"<Button-1>\", lambda event: webbrowser.open(link.cget(\"text\")))       \n",
    "        elif i==1:\n",
    "            link1 = Label(canvas, text=liens[i], fg=\"#116562\", cursor=\"hand2\")\n",
    "            link1.grid(row =i, column =1,padx=5, pady=5)\n",
    "            link1.bind(\"<Button-1>\", lambda event: webbrowser.open(link1.cget(\"text\")))\n",
    "        elif i==2:\n",
    "            link2 = Label(canvas, text=liens[i], fg=\"#116562\", cursor=\"hand2\")\n",
    "            link2.grid(row =i, column =1,padx=5, pady=5)\n",
    "            link2.bind(\"<Button-1>\", lambda event: webbrowser.open(link2.cget(\"text\")))\n",
    "        elif i==3:\n",
    "            link3 = Label(canvas, text=liens[i], fg=\"#116562\", cursor=\"hand2\")\n",
    "            link3.grid(row =i, column =1,padx=5, pady=5)\n",
    "            link3.bind(\"<Button-1>\", lambda event: webbrowser.open(link3.cget(\"text\")))\n",
    "        elif i==4:\n",
    "            link4 = Label(canvas, text=liens[i], fg=\"#116562\", cursor=\"hand2\")\n",
    "            link4.grid(row =i, column =1,padx=5, pady=5)\n",
    "            link4.bind(\"<Button-1>\", lambda event: webbrowser.open(link4.cget(\"text\")))\n",
    "        elif i==5:\n",
    "            link5 = Label(canvas, text=liens[i], fg=\"#116562\", cursor=\"hand2\")\n",
    "            link5.grid(row =i, column =1,padx=5, pady=5)\n",
    "            link5.bind(\"<Button-1>\", lambda event: webbrowser.open(link5.cget(\"text\")))\n",
    "        elif i==6:\n",
    "            link6 = Label(canvas, text=liens[i], fg=\"#116562\", cursor=\"hand2\")\n",
    "            link6.grid(row =i, column =1,padx=5, pady=5)\n",
    "            link6.bind(\"<Button-1>\", lambda event: webbrowser.open(link6.cget(\"text\")))\n",
    "        elif i==7:\n",
    "            link7 = Label(canvas, text=liens[i], fg=\"#116562\", cursor=\"hand2\")\n",
    "            link7.grid(row =i, column =1,padx=5, pady=5)\n",
    "            link7.bind(\"<Button-1>\", lambda event: webbrowser.open(link7.cget(\"text\")))\n",
    "        elif i==8:\n",
    "            link8 = Label(canvas, text=liens[i], fg=\"#116562\", cursor=\"hand2\")\n",
    "            link8.grid(row =i, column =1,padx=5, pady=5)\n",
    "            link8.bind(\"<Button-1>\", lambda event: webbrowser.open(link8.cget(\"text\")))\n",
    "        else :\n",
    "            link9 = Label(canvas, text=liens[i], fg=\"#116562\", cursor=\"hand2\")\n",
    "            link9.grid(row =i, column =1,padx=5, pady=5)\n",
    "            link9.bind(\"<Button-1>\", lambda event: webbrowser.open(link9.cget(\"text\")))\n",
    "#from PyQt4 import QtGui\n",
    "from tkinter import * \n",
    "fenetre = Tk() # ouvre une instance de fenêtre\n",
    "fenetre.geometry('900x600')\n",
    "fenetre.title(\"Web Sementique ~ Moteur de recherche\")\n",
    "\n",
    "#set window color\n",
    "#fenetre.configure(bg ='cadetblue', bd =2, relief=SUNKEN)\n",
    "fenetre.config(bg='#116562')\n",
    "\n",
    "# set minimum window size value\n",
    "fenetre.minsize(900, 600)\n",
    " \n",
    "# set maximum window size value\n",
    "fenetre.maxsize(900, 600)\n",
    "\n",
    "\"\"\"\n",
    "#Placer l'interface au Centre\n",
    "\n",
    "def center(toplevel):\n",
    "    toplevel.update_idletasks()\n",
    "\n",
    "    # Tkinter way to find the screen resolution\n",
    "    # screen_width = toplevel.winfo_screenwidth()\n",
    "    # screen_height = toplevel.winfo_screenheight()\n",
    "\n",
    "    # PyQt way to find the screen resolution\n",
    "    app = QtGui.QApplication([])\n",
    "    screen_width = app.desktop().screenGeometry().width()\n",
    "    screen_height = app.desktop().screenGeometry().height()\n",
    "\n",
    "    size = tuple(int(_) for _ in toplevel.geometry().split('+')[0].split('x'))\n",
    "    x = screen_width/2 - size[0]/2\n",
    "    y = screen_height/2 - size[1]/2\n",
    "\n",
    "    toplevel.geometry(\"+%d+%d\" % (x, y))\n",
    "    toplevel.title(\"Centered!\")\n",
    "    \n",
    "win = Tk.Toplevel(fenetre)\n",
    "center(win)\n",
    "\"\"\"\n",
    "\n",
    "label = Label(fenetre, text=\"Entity Disambiguation \",font=\"Arial 16 italic\",bg=\"#116562\") # fg=\"red\"\n",
    "label.pack(side=TOP,padx=30, pady=30)\n",
    "value = StringVar() # DoubleVar 0.0 , IntVar 0, StringVar ‘’\n",
    "value.set(\"\")\n",
    "entree = Entry(fenetre, textvariable=value,font=\"Arial 16 italic\", width=150) \n",
    "entree.pack(padx=60, pady=25)\n",
    "entree.insert(0,\"Please insert your sentence here...\")\n",
    "entree.pack(padx=25, pady=25)\n",
    "entree.configure(state=DISABLED)\n",
    "def on_click(event):\n",
    "    entree.configure(state=NORMAL)\n",
    "    entree.delete(0, END)\n",
    "    # make the callback only work once\n",
    "    entree.unbind('<Button-1>', on_click_id)\n",
    "\n",
    "on_click_id = entree.bind('<Button-1>', on_click)\n",
    "from tkinter import font\n",
    "myFont = font.Font(size=15)\n",
    "b1=Button(fenetre , text=\"Search\",bg='cadetblue', fg='#ffffff',font=\"Arial 16 italic\", command=recherche)\n",
    "b1['font'] = myFont\n",
    "\n",
    "b1.pack(side=TOP, padx=7, pady=7)\n",
    "canvas = Canvas(fenetre, bg='white',width=550, height=320 )\n",
    "canvas.pack() \n",
    "\n",
    "fenetre.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a30c5e8",
   "metadata": {},
   "source": [
    "# Sebastian Thrun works at Google\n",
    "# Oracle Database\n",
    "# luxury vehicle brand of Jaguar Land Rover\n",
    "# Cristiano Ronaldo\n",
    "# Messi\n",
    "# Mac OS X Jaguar is a good operating System "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e708d01c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
